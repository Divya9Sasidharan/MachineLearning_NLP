# -*- coding: utf-8 -*-
"""Gutenberg_Genre_IdentificationNotebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11FAVtUUbkEeSuVuXfkU3nOBf_8xIw1ES
"""

import os
os.getcwd()
import pandas as pd
import glob
import os
import re
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.chunk import tree2conlltags
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.classify import apply_features
import numpy as np
from sklearn.model_selection import train_test_split
from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

import numpy as np
# import cv2
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from wordcloud import WordCloud
from sklearn import datasets
from sklearn import metrics
from sklearn.metrics import roc_curve, auc, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as make_pipeline_imb

import os
os.getcwd()

"""# Data exploration and preparation"""

import pandas as pd
df = pd.read_csv('C:\\Users\\Admin\\Desktop\\Gutenberg_English_Fiction_1k\\master996.csv', delimiter=';', encoding= 'unicode_escape') #original csv
df

#remove .epub extension from book_id
df['book_id'] = df['book_id'].str[:-5]
df
#df.to_excel('test.xlsx', sheet_name='sheet1', index=False)

print("Number of genres:", df.guten_genre.nunique())

print("Genres:")
df.guten_genre.unique()

print("Number of books in the dataset for each genre:") #note the class imbalance
df.guten_genre.value_counts()

import glob
import os
#get data
file_list = glob.glob(os.path.join(os.getcwd(), "Gutenberg_19th_century_English_Fiction", "*.html")) #contains path to file
ids_book = []
corpus = []
count = 0
for file_path in file_list:
  if os.path.getsize(file_path) > 0: #ignore empty files
    name = os.path.basename(file_path)
    newFilename = name.replace("-content.html", "")
    if newFilename in df.book_id.values: #only add books to the list which are there in csv
      with open(file_path, encoding='UTF-8') as f_input:
          count+=1
          corpus.append(f_input.read())
          ids_book.append(newFilename)
          print("appending file {} count {}".format(name, count)) #there are 996 books in total in the given csv file
  else:
     print("Empty file:"+format(file_path))

print(ids_book)

print(len(ids_book))

print(corpus[0]) #view raw data

#Remove html tags from text
import re
corpus_new = []
def remove_html_tags(text):
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)
for i in range(len(corpus)):
    corpus_new.append(remove_html_tags(corpus[i]))

print(corpus_new[0])

#target names
target_names = []
target_names = df.guten_genre.unique().tolist()
print(target_names)

genre_from_csv = []
print(ids_book)
for i in ids_book:
  genre = df.loc[df['book_id'] == i, 'guten_genre'].iloc[0]
  genre_from_csv.append(genre)
print(genre_from_csv)

"""# Feature extraction"""

# import library
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('vader_lexicon') #for sentiment analysis
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('maxent_ne_chunker')
nltk.download('words')
from nltk.chunk import tree2conlltags
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer

#extract female propernoun feature from corpus
femalePropN_list=['she']
def femalePropN(corpus_lemmas):
  count = 0  
  featureCount={}
  for lemmas in corpus_lemmas:
    if lemmas in femalePropN_list:
      count= count+1  
  featureCount['femalePropN']=count
  return featureCount

#extract male propernoun feature from corpus
malePropN_list=['he']
def malePropN(corpus_lemmas):
  count = 0  
  featureCount={}
  for lemmas in corpus_lemmas:
    if lemmas in malePropN_list:
      count= count+1
  featureCount['malePropN']=count
  return featureCount

#extract count of paragraphs
def countParas(corpus):
  featureCount={}
  num_paras = len(corpus.split("\n"))
  featureCount['countParas']=num_paras
  return featureCount
  
#extract the average length of sentences
def avgSentsLength(corpus, tokens):
  featureCount={}
  num_sents = len(sent_tokenize(corpus))  #to get number of sentences
  total_len_sent = len(tokens)  #adding all sentence length to get total
  if num_sents != 0: #because book pg38685 is empty
    avg_sent_length = int(total_len_sent/num_sents)
    featureCount['avgSentsLength']=avg_sent_length        
  else:
    featureCount['avgSentsLength']=0
  return featureCount

#extract the pos tag count for each corpus file
def pos_tag(corpus):  
  word_tag=nltk.pos_tag(corpus)
  featureCount={}
  for word_tag_itr in word_tag:
    if word_tag_itr[1]  in featureCount:
      featureCount[word_tag_itr[1]]=featureCount[word_tag_itr[1]]+1
    else:
      featureCount[word_tag_itr[1]]=1
  return featureCount

#extract semi colon count
def semiColonCount(tokens):  
  featureCount = {}
  count = 0
  for t in range(len(tokens)):          
    if ';' in tokens[t]:      
      count = count + 1 
  featureCount['semiColonCount'] = count 
  return featureCount

#extract named entities
def namedEntityExtractor(tuple1):
    featureCount = {}
    #bring the tuple back to lists to work with it
    if len(tuple1) != 0:
      words, tags, pos = zip(*tuple1)
      words = list(words)
      pos = list(pos)
      c_persons = list()
      c_gpe = list()
      i=0
      while i<= len(tuple1)-1:
          if pos[i] == 'B-PERSON':
              c_persons = c_persons + [words[i]]
              featureCount['persons_count'] = len(set(c_persons))
          elif pos[i] == 'B-GPE':
              c_gpe = c_gpe + [words[i]]
              featureCount['location_count'] = len(set(c_gpe))
          i=i+1
    
    else:
      featureCount['persons_count'] = 0
      featureCount['location_count'] = 0
      
    return featureCount

def feature_extractor(corpus):
  #tokenizer = RegexpTokenizer('\w+')
  lemmas = []
  feature_set_f=[]
  for i in range(len(corpus)):
    #feature_set=[]  
    wordpunc_tokens = word_tokenize(corpus[i]) #words + punctuations
    lemmatized_words = ' '.join([WordNetLemmatizer().lemmatize(w) for w in wordpunc_tokens])
    lemmas = lemmatized_words.split(' ')
    feature={}
    out=femalePropN(lemmas)# 1st feature extraction   
    feature.update(out) 
    out=malePropN(lemmas) # 2nd feature extraction   
    feature.update(out)
    out = countParas(corpus[i]) # 3rd feature extraction  
    feature.update(out)
    out = avgSentsLength(corpus[i], wordpunc_tokens) # 4th feature extraction  
    feature.update(out)
    out = semiColonCount(wordpunc_tokens) # 5th feature extraction  
    feature.update(out)
    out = pos_tag(wordpunc_tokens) # 6th feature extraction 
    feature.update(out)
    out = namedEntityExtractor(tree2conlltags(nltk.ne_chunk(nltk.pos_tag(wordpunc_tokens)))) # 7th feature extraction
    feature.update(out)
    out = SentimentIntensityAnalyzer().polarity_scores(corpus[i]) # 8th feature extraction 
    feature.update(out)
    feature_set_f.append(feature)
    print(i)
  return feature_set_f

from nltk.classify import apply_features
feature_set = feature_extractor(corpus_new)

file1 = open("featureset236.txt", "w")
print(feature_set)
file1.write(str(feature_set))
file1.close()

len(feature_set)

"""# Modeling"""

import numpy as np
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(feature_set, genre_from_csv, test_size=0.30, stratify = genre_from_csv)

file2 = open("ytest.txt", "w")
print(y_test)
file2.write(str(y_test))
file2.close()

file5 = open("ytrain.txt", "w")
file5.write(str(y_train))
file5.close()

train_set = []
test_set = []
for i in range(len(X_train)):
    train = []
    train.append(X_train[i])
    train.append(y_train[i])
    train_set.append(tuple(train))

for i in range(len(X_test)):
    test = []
    test.append(X_test[i])
    test.append(y_test[i])
    test_set.append(tuple(test))

len(train_set)

file3 = open("trainset.txt", "w")
print(y_test)
file3.write(str(train_set))
file3.close()

file4 = open("testset.txt", "w")
print(y_test)
file4.write(str(test_set))
file4.close()

"""# Naive Bayes classifier"""

classifier = nltk.NaiveBayesClassifier.train(train_set)

print(nltk.classify.accuracy(classifier, test_set))

classifier.show_most_informative_features(10)

"""# Decision Tree classifer"""

classifier = nltk.DecisionTreeClassifier.train(train_set)

nltk.classify.accuracy(classifier, test_set)

"""# Bernoulli Naive Bayes classifier"""

from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
classif = SklearnClassifier(BernoulliNB()).train(train_set)

nltk.classify.accuracy(classif, test_set)

"""# SVC classifier"""

classifi = SklearnClassifier(SVC(), sparse=False).train((train_set))

nltk.classify.accuracy(classifi, test_set)

"""# Random Forest classifier"""

from sklearn.ensemble import RandomForestClassifier
clf = SklearnClassifier(RandomForestClassifier()).train((train_set))

nltk.classify.accuracy(clf, test_set)

"""# Confusion matrix -- Random forest classifier"""

refsets = []
testsets = []
for i, (feats, label) in enumerate(test_set):
  refsets.append(label)
  observed = clf.classify(feats)
  testsets.append(observed)

refsets

testsets

cm = nltk.ConfusionMatrix(refsets, testsets)
print(cm.pretty_format(sort_by_count=True, show_percents=True))

"""# Feature selection"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.getcwd()
os.chdir("/content/drive/My Drive/Colab Notebooks")

import ast

with open("featureset236.txt", 'r') as f:
    mylist = ast.literal_eval(f.read())

mylist

import pandas as pd
df = pd.DataFrame(mylist)
df.head(3)

books_genre = ['Detective and Mystery', 'Literary', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Ghost and Horror', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Sea and Adventure', 'Literary', 'Literary', 'Christmas Stories', 'Love and Romance', 'Literary', 'Literary', 'Love and Romance', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Allegories', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Detective and Mystery', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Christmas Stories', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Love and Romance', 'Detective and Mystery', 'Sea and Adventure', 'Sea and Adventure', 'Detective and Mystery', 'Detective and Mystery', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Sea and Adventure', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Sea and Adventure', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Allegories', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Ghost and Horror', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Western Stories', 'Literary', 'Detective and Mystery', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Ghost and Horror', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Humorous and Wit and Satire', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Sea and Adventure', 'Literary', 'Western Stories', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Humorous and Wit and Satire', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Humorous and Wit and Satire', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Humorous and Wit and Satire', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Ghost and Horror', 'Detective and Mystery', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Sea and Adventure', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Western Stories', 'Western Stories', 'Literary', 'Literary', 'Literary', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Christmas Stories', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Sea and Adventure', 'Ghost and Horror', 'Literary', 'Christmas Stories', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Detective and Mystery', 'Literary', 'Western Stories', 'Literary', 'Western Stories', 'Literary', 'Literary', 'Western Stories', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Western Stories', 'Literary', 'Love and Romance', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Western Stories', 'Detective and Mystery', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Detective and Mystery', 'Literary', 'Christmas Stories', 'Literary', 'Literary', 'Literary', 'Literary', 'Humorous and Wit and Satire', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Humorous and Wit and Satire', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Ghost and Horror', 'Detective and Mystery', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Love and Romance', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Literary', 'Detective and Mystery', 'Literary', 'Detective and Mystery', 'Literary', 'Sea and Adventure', 'Literary', 'Literary', 'Sea and Adventure', 'Literary', 'Literary']

genre = pd.DataFrame(books_genre)

df.insert(loc = 0, column = 'guten_genre', value = genre)

df.head(3)

# check how many null values are in our dataset
df.isnull().sum()

# drop those features which are non-sensible or have many NAN values
df.drop(['WP$', 'SYM', 'FW', 'NNPS', '(', ')', '#', '$', 'LS', ','], axis = 1, inplace = True)

df.head(3)

# Get mean value of each column to fill NAN values of respective column
mean_values = [0]
for i in range(1, 47):
    m_value = df.iloc[:, i:i+1].mean()[0]
    mean_values.append(m_value)

# create a dictionary of column name and mean values to pass it in fillna method
columns = df.columns.values
dic_nan = dict(zip(columns, mean_values))

# filling all NAN values with mean value
df.fillna(value = dic_nan, inplace = True)

# make sure no NAN values remain
df.isnull().sum()

"""# Modeling - after feature selection"""

X = df.iloc[:,1:46]
y = df.iloc[:,0:1]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)

"""# Multinomial Naive Bayes"""

import numpy as np
# import cv2
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from wordcloud import WordCloud
from sklearn import datasets
from sklearn import metrics
from sklearn.metrics import roc_curve, auc, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as make_pipeline_imb

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()

clf.fit(X_train, y_train)

predicted = clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))

!pip install -q scikit-plot
import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(y_test, predicted, figsize=(7,5), x_tick_rotation = 90)

"""# Modeling after Over-sampling with SMOTE"""

! pip install -U imbalanced-learn

# Over_sampling minority classes
sm = SMOTE(random_state = 42, k_neighbors = 1)
X_res, y_res = sm.fit_sample(X, y)

y_res.guten_genre.value_counts()

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.20)



"""## Naive Bayes Classifier"""

clf = MultinomialNB()

clf.fit(X_train, y_train.values.ravel())

predicted = clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))

skplt.metrics.plot_confusion_matrix(y_test, predicted, figsize=(7,5), x_tick_rotation = 90)

scores = cross_val_score(clf, X_res, y_res.values.ravel(), cv=5)

print("Accuracy: {}".format(scores.mean()))

"""## Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=0)

clf.fit(X_train, y_train.values.ravel())

predicted = clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))

skplt.metrics.plot_confusion_matrix(y_test, predicted, figsize=(7,5), x_tick_rotation = 90)

scores = cross_val_score(clf, X_res, y_res.values.ravel(), cv=5)

print("Accuracy: {}".format(scores.mean()))

"""## Random Forest Classsifier"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier()

clf.fit(X_train, y_train.values.ravel())

predicted = clf.predict(X_test)

print(metrics.classification_report(y_test, predicted))

skplt.metrics.plot_confusion_matrix(y_test, predicted, figsize=(7,5), x_tick_rotation = 90)

scores = cross_val_score(clf, X_res, y_res.values.ravel(), cv=5)

print("Accuracy: {}".format(scores.mean()))